# 集群

ES 的集群搭建很简单，不需要依赖第三方协调管理组件，自身内部就实现了集群的管理功能。ES 集群由一个或多个 Elasticsearch 节点组成，每个节点配置相同的 `cluster.name` 即可加入集群，默认值为 `elasticsearch`。确保不同的环境中使用不同的集群名称，否则最终会导致节点加入错误的集群。

一个 Elasticsearch 服务启动实例就是一个节点（Node）。节点通过 `node.name` 来设置节点名称，如果不设置则在启动时给节点分配一个随机通用唯一标识符作为名称。

## 发现机制

那么有一个问题，ES 内部是如何通过一个相同的设置 `cluster.name` 就能将不同的节点连接到同一个集群的？答案是 `Zen Discovery`。

Zen Discovery 是 Elasticsearch 的内置默认发现模块（发现模块的职责是发现集群中的节点以及选举master节点）。它提供单播和基于文件的发现，并且可以扩展为通过插件支持云环境和其他形式的发现。Zen Discovery 与其他模块集成，例如，节点之间的所有通信都使用[Transport模块](https://www.elastic.co/guide/en/elasticsearch/reference/6.8/modules-transport.html)完成。节点使用发现机制通过Ping 的方式查找其他节点。

Elasticsearch 默认被配置为使用单播发现，以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。

如果集群的节点运行在不同的机器上，使用单播，可以为 Elasticsearch 提供一些它应该去尝试连接的节点列表。 当一个节点联系到单播列表中的成员时，它就会得到整个集群所有节点的状态，然后它会联系 master 节点，并加入集群。

这意味着单播列表不需要包含集群中的所有节点， 它只是需要足够的节点，当一个新节点联系上其中一个并且说上话就可以了。如果使用 master 候选节点作为单播列表，只要列出三个就可以了。 这个配置在 `elasticsearch.yml` 文件中：

```yml
discovery.zen.ping.unicast.hosts: ["host1", "host2:port"]
```

节点启动后先 ping ，如果 `discovery.zen.ping.unicast.hosts` 有设置，则 ping 设置中的 host，否则尝试 ping localhost 的几个端口， Elasticsearch 支持同一个主机启动多个节点， Ping 的 response 会包含该节点的基本信息以及该节点认为的 master 节点。 选举开始，先从各节点认为的 master 中选，规则很简单，按照 id 的字典序排序，取第一个。 如果各节点都没有认为的 master，则从所有节点中选择，规则同上。

这里有个限制条件就是 `discovery.zen.minimum_master_nodes`，如果节点数达不到最小值的限制，则循环上述过程，直到节点数足够可以开始选举。 最后选举结果是肯定能选举出一个 master，如果只有一个 local 节点那就选出的是自己。 如果当前节点是 master ，则开始等待节点数达到 `discovery.zen.minimum_master_nodes`，然后提供服务。 如果当前节点不是 master ，则尝试加入 master。Elasticsearch 将以上服务发现以及选主的流程叫做 ZenDiscovery 。

由于它支持任意数目的集群（1 ~ N），所以不能像 Zookeeper 那样限制节点必须是奇数，也就无法用投票的机制来选主，而是通过一个规则，只要所有的节点都遵循同样的规则，得到的信息都是对等的，选出来的主节点肯定是一致的。但分布式系统的问题就出在信息不对等的情况，这时候很容易出现脑裂（Split-Brain）的问题，大多数解决方案就是设置一个 quorum 值，要求可用节点必须大于 quorum （一般是超过半数节点），才能对外提供服务。而 Elasticsearch 中，这个 quorum 的配置就是 `discovery.zen.minimum_master_nodes`。

## 节点的角色

每个节点既可以是**候选主节点**也可以是**数据节点**，通过在配置文件 `../config/elasticsearch.yml` 中设置即可，默认都为`true`。

```yml
node.master: true  //是否候选主节点
node.data: true    //是否数据节点
```

**数据节点**负责数据的存储和相关的操作，例如对数据进行增、删、改、查和聚合等操作，所以数据节点（data节点）对机器配置要求比较高，对CPU、内存和I/O的消耗很大。通常随着集群的扩大，需要增加更多的数据节点来提高性能和可用性。

**候选主节点**可以被选举为主节点（master节点），集群中只有候选主节点才有选举权和被选举权，其他节点不参与选举的工作。主节点负责创建索引、删除索引、跟踪哪些节点是群集的一部分，并决定哪些分片分配给相关的节点、追踪集群中节点的状态等，稳定的主节点对集群的健康是非常重要的。

![img](https://i.loli.net/2020/04/01/P9ESwGAOCBMmcgn.png)

一个节点既可以是候选主节点也可以是数据节点，但是由于数据节点对 CPU、内存核 I/O 消耗都很大，所以如果某个节点既是数据节点又是主节点，那么可能会对主节点产生影响从而对整个集群的状态产生影响。

因此为了提高集群的健康性，应该对 Elasticsearch 集群中的节点做好角色上的划分和隔离。可以使用几个配置较低的机器群作为候选主节点群。

主节点和其他节点之间通过 Ping 的方式互检查，主节点负责 Ping 所有其他节点，判断是否有节点已经挂掉。其他节点也通过 Ping 的方式判断主节点是否处于可用状态。

虽然对节点做了角色区分，但是用户的请求可以发往任何一个节点，并由该节点负责分发请求、收集结果等操作，而不需要主节点转发，这种节点可称之为**协调节点**，协调节点是不需要指定和配置的，集群中的任何节点都可以充当协调节点的角色。

## 脑裂现象

同时如果由于网络或其他原因导致集群中选举出多个Master节点，使得数据更新时出现不一致，这种现象称之为**脑裂**，即集群中不同的节点对于 master 的选择出现了分歧，出现了多个 master 竞争。

“脑裂”问题可能有以下几个原因造成：

1. **网络问题**：集群间的网络延迟导致一些节点访问不到 master，认为 master 挂掉了从而选举出新的master，并对 master 上的分片和副本标红，分配新的主分片
2. **节点负载**：主节点的角色既为 master 又为 data，访问量较大时可能会导致ES停止响应（假死状态）造成大面积延迟，此时其他节点得不到主节点的响应认为主节点挂掉了，会重新选取主节点。
3. **内存回收**：主节点的角色既为 master 又为 data，当 data 节点上的 ES 进程占用的内存较大，引发 JVM 的大规模内存回收，造成 ES 进程失去响应。

为了避免脑裂现象的发生，我们可以从原因着手通过以下几个方面来做出优化措施：

1. **适当调大响应时间，减少误判**
   通过参数 `discovery.zen.ping_timeout` 设置节点状态的响应时间，默认为 3s，可以适当调大，如果 master 在该响应时间的范围内没有做出响应应答，判断该节点已经挂掉了。调大参数（如 6s，`discovery.zen.ping_timeout:6`），可适当减少误判。
2. **选举触发**
   需要在候选集群中的节点的配置文件中设置参数 `discovery.zen.munimum_master_nodes` 的值，这个参数表示在选举主节点时需要参与选举的候选主节点的节点数，默认值是1，官方建议取值 `(master_eligibel_nodes/2) + 1`，其中 `master_eligibel_nodes` 为候选主节点的个数。这样做既能防止脑裂现象的发生，也能最大限度地提升集群的高可用性，因为只要不少于*`discovery.zen.munimum_master_nodes`* 个候选节点存活，选举工作就能正常进行。当小于这个值的时候，无法触发选举行为，集群无法使用，不会造成分片混乱的情况。
3. **角色分离**
   即是上面我们提到的候选主节点和数据节点进行角色分离，这样可以减轻主节点的负担，防止主节点的假死状态发生，减少对主节点“已死”的误判。